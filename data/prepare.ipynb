{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "import pyproj\n",
    "import netCDF4\n",
    "import pandas\n",
    "\n",
    "PSMSLNAME=re.compile(r'(?P<id>\\d+)\\.rlrdata$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSMSL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open the zip file\n",
    "zf = zipfile.ZipFile('rlr_monthly.zip')\n",
    "# create dataframes for each time series\n",
    "dfs = []\n",
    "for info in zf.filelist:\n",
    "    # only loop over data\n",
    "    if not info.filename.endswith('rlrdata'):\n",
    "        continue\n",
    "    # open the file\n",
    "    f = zf.open(info.filename)\n",
    "    # load it as a csv file\n",
    "    df = pandas.read_csv(f, sep=';', header=None, index_col=None)\n",
    "    # set the columns (found in matlab parser)\n",
    "    df.columns = ['year.month', 'waterlevel', 'missing', 'dataflag']\n",
    "    id = int(PSMSLNAME.search(info.filename).group('id'))\n",
    "    df[\"id\"] = id\n",
    "    dfs.append(df)\n",
    "# group \n",
    "series_df = pandas.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the overview\n",
    "f = zf.open('rlr_monthly/filelist.txt')\n",
    "overview_df = pandas.DataFrame.from_csv(f, sep=';', header=None, index_col=None)\n",
    "overview_df.columns = ['id', 'latitude', 'longitude', 'name', 'coastline', 'stationcode', 'stationflag']         \n",
    "overview_df[\"name\"] = overview_df[\"name\"].apply(str.strip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for stationfile in os.listdir('extra'):\n",
    "    if stationfile == 'stations.csv':\n",
    "        continue\n",
    "    df = pandas.read_csv(os.path.join('extra', stationfile))\n",
    "    dfs.append(df)\n",
    "series_df = pandas.concat(dfs + [series_df.copy()])\n",
    "\n",
    "df = pandas.read_csv(os.path.join('extra', 'stations.csv'))\n",
    "overview_df = overview_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# curry the time series\n",
    "# No missings in ints (http://pandas.pydata.org/pandas-docs/stable/gotchas.html)\n",
    "series_df[\"waterlevel\"] = series_df.waterlevel.astype('double')\n",
    "series_df[\"waterlevel\"][series_df[\"waterlevel\"] == -99999] = None\n",
    "# date time transformation\n",
    "series_df[\"year\"] = np.floor(series_df[\"year.month\"])\n",
    "overview_df = overview_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# avoid memory error?!\n",
    "series_df[\"month\"] = np.floor((np.array(series_df[\"year.month\"])- np.array(series_df[\"year\"]))*12 )+1\n",
    "# make datetimes\n",
    "f = lambda x: datetime.datetime(int(x[\"year\"]), int(x[\"month\"]),1)\n",
    "series_df[\"time\"] = pandas.to_datetime(series_df[[\"year\", \"month\"]].apply(f, axis=1))\n",
    "# lookup  station id\n",
    "\n",
    "data_grouped = series_df.groupby([\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Lat', u'Lon')\n"
     ]
    }
   ],
   "source": [
    "# Read the GIA file\n",
    "ds = netCDF4.Dataset('./dsea250.1grid.ICE5Gv1.3_VM2_L90_2012.nc')\n",
    "lon = ds.variables['Lon'][:]\n",
    "lat = ds.variables['Lat'][:]\n",
    "dsea = ds.variables['Dsea_250'][:]\n",
    "print(ds.variables['Dsea_250'].dimensions)\n",
    "ds.close()\n",
    "# flip for increasing lat\n",
    "dsea = dsea[::-1,:] # flipud\n",
    "lon = np.mod(np.deg2rad(lon), np.pi*2)\n",
    "lat = np.deg2rad(lat[::-1]) + 0.5*np.pi # flip\n",
    "# interpolate on the earth\n",
    "F = scipy.interpolate.RectSphereBivariateSpline(u=lat, v=lon, r=dsea)\n",
    "overview_df[\"peltier\"] = F.ev(np.deg2rad(overview_df.latitude) + 0.5*np.pi, np.mod(np.deg2rad(overview_df.longitude), np.pi*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'time', u'lat', u'lon')\n"
     ]
    }
   ],
   "source": [
    "# Read wind reanalysis\n",
    "ds = netCDF4.Dataset('uwnd.10m.mon.mean.nc')\n",
    "lon = ds.variables['lon'][:]\n",
    "lat = ds.variables['lat'][:]\n",
    "u = ds.variables['uwnd'][:]\n",
    "time = netCDF4.num2date(ds.variables['time'][:], ds.variables['time'].units)\n",
    "print(ds.variables['uwnd'].dimensions)\n",
    "ds.close()\n",
    "lon = np.mod(np.deg2rad(lon), np.pi*2)\n",
    "lat = np.deg2rad(lat[::-1]) + 0.5*np.pi # flip\n",
    "u = u[:,::-1,:]\n",
    "ds = netCDF4.Dataset('vwnd.10m.mon.mean.nc')\n",
    "v = ds.variables['vwnd'][:]\n",
    "ds.close()\n",
    "v = v[:,::-1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpolate on the earth, to each station\n",
    "dfs = []\n",
    "for i, t in enumerate(time):\n",
    "    F_i = scipy.interpolate.RectSphereBivariateSpline(u=lat, v=lon, r=u[i,:,:])\n",
    "    u_i = F_i.ev(np.deg2rad(overview_df.latitude) + 0.5*np.pi, np.mod(np.deg2rad(overview_df.longitude), np.pi*2))\n",
    "    F_i = scipy.interpolate.RectSphereBivariateSpline(u=lat, v=lon, r=v[i,:,:])    \n",
    "    v_i = F_i.ev(np.deg2rad(overview_df.latitude) + 0.5*np.pi, np.mod(np.deg2rad(overview_df.longitude), np.pi*2))\n",
    "    df = overview_df[[]].reset_index()\n",
    "    df[\"u_i\"] = u_i\n",
    "    df[\"v_i\"] = v_i\n",
    "    df[\"time\"] = t\n",
    "    dfs.append(df)\n",
    "df = pandas.concat(dfs)\n",
    "uv_grouped = df.groupby([\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse barometer correction\n",
    "=============================\n",
    "\n",
    "IB (mm) = -9.948 * ( d Rdry (mbars) - 1013.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'time', u'lat', u'lon')\n"
     ]
    }
   ],
   "source": [
    "# Do an inverse barometer correction\n",
    "ds = netCDF4.Dataset('./slp.mnmean.real.nc')\n",
    "time = netCDF4.num2date(ds.variables['time'][:],ds.variables['time'].units)\n",
    "lon = ds.variables['lon'][:].astype('double')\n",
    "lat = ds.variables['lat'][:].astype('double')\n",
    "slp = ds.variables['slp'][:].astype('double')\n",
    "print(ds.variables['slp'].dimensions)\n",
    "ds.close()\n",
    "\n",
    "ds = netCDF4.Dataset(\"landmask.nc\")\n",
    "landmask = ds.variables[\"mask\"][:][::-1,:].astype(\"bool\") # no need to flip\n",
    "ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slp_mask=np.tile(landmask[np.newaxis, :,:], (slp.shape[0], 1,1))\n",
    "slp_masked = np.ma.masked_array(slp, mask=slp_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "area_weight = np.ma.masked_array(\n",
    "            np.tile(np.cos(np.rad2deg(lat))[np.newaxis,:,np.newaxis], (slp_masked.shape[0], 1, slp_masked.shape[2])),\n",
    "            slp_mask        \n",
    "                    )\n",
    "glob_p = (slp_masked*area_weight).sum(1).sum(1)\n",
    "glob_p /= area_weight.sum(1).sum(1)\n",
    "\n",
    "IB = -(slp_masked-np.tile(glob_p[:,np.newaxis, np.newaxis], (1,slp_masked.shape[1],slp_masked.shape[2])))*0.9948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lookup closest IB for each station (interpolation breaks because of the landmasks)\n",
    "wgs84 = pyproj.Geod(ellps='WGS84')\n",
    "Lon, Lat = np.meshgrid(lon, lat)\n",
    "\n",
    "lon1, lat1 = Lon.ravel(), Lat.ravel()\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for id, row in overview_df.iterrows():\n",
    "    lon2, lat2 = np.ones_like(Lon.ravel())*row['longitude'], np.ones_like(Lat.ravel())*row['latitude']\n",
    "    az1, az2, dist = wgs84.inv(lon1, lat1, lon2, lat2)\n",
    "    dist_masked = np.ma.masked_array(dist.reshape(Lon.shape), mask=landmask[:,:])\n",
    "    idx = np.unravel_index(dist_masked.argmin(), dist_masked.shape)\n",
    "    ib = IB[:, idx[0], idx[1]]\n",
    "    df = pandas.DataFrame(data=dict(time=time, ib=ib))\n",
    "    df['id'] = id\n",
    "    dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ib_dfs = pandas.concat(dfs)\n",
    "ib_grouped = ib_dfs.groupby(['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merges = []\n",
    "for station, row in overview_df.iterrows():\n",
    "    uv = uv_grouped.get_group(station)\n",
    "    ib = ib_grouped.get_group(station)\n",
    "    data = data_grouped.get_group(station)\n",
    "    merged = pandas.merge(data, ib, how=\"left\", on=(\"time\",\"id\"))\n",
    "    merged = pandas.merge(merged, uv, how=\"left\", on=(\"time\", \"id\"))\n",
    "    merges.append(merged)\n",
    "dataset = pandas.concat(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset[\"u2\"] = dataset[\"u_i\"]**2\n",
    "dataset[\"v2\"] = dataset[\"v_i\"]**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = dataset[['id', 'year', 'waterlevel', 'ib', 'u_i', 'v_i', \"u2\", \"v2\"]]\n",
    "year_df = dataset[[\"id\",\"year\", \"waterlevel\", \"ib\", \"u_i\",\"v_i\", \"u2\", \"v2\"]].groupby(['id', 'year']).mean()\n",
    "year_df = year_df.reset_index()\n",
    "year_df.rename(columns={'year': 'year.month'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"records.csv\", index=False)\n",
    "year_df.to_csv(\"records_annual.csv\", index=False)\n",
    "overview_df.to_csv(\"overview.csv\", index=True, index_label=\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
